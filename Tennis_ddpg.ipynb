{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\t\nUnity brain name: TennisBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 8\n        Number of stacked Vector Observation: 3\n        Vector Action space type: continuous\n        Vector Action space size (per agent): 2\n        Vector Action descriptions: , \n"
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./environment/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of agents: 2\nSize of each action: 2\nThere are 2 agents. Each observes a state with length: 24\nThe state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.         -6.65278625 -1.5\n -0.          0.          6.83172083  6.         -0.          0.        ]\n"
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[False, False]"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "env_info.local_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(agent, n_episodes=300, max_t=1000, stop_if_solved=True, abort_threshold=None):\n",
    "    \"\"\"MADDPG training.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        agent (Agent): the MADDPG agent\n",
    "        starting_episode (int):  the starting episode number\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        stop_if_solved (boolean): true, if training has to stop when the agent solved the environment\n",
    "        abort_threshold (int): abort training if the agent has scored high enough untis this episode\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores \n",
    "\n",
    "    # beta is only used by agents using PER\n",
    "    beta_min = 0.4\n",
    "    noise_min = 0.1\n",
    "\n",
    "    solved = False\n",
    "\n",
    "    def next_discount(t, min_value):\n",
    "        return min(1.0, min_value + t * (1.0 - min_value) / max_t)\n",
    "\n",
    "    # noise reduction\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        agent.reset()\n",
    "        states = env_info.vector_observations\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action1 = agent.act(states[0])\n",
    "            action2 = agent.act(states[1])\n",
    "            actions = np.concatenate((action1, action2))\n",
    "            env_info = env.step(actions)[brain_name]           # send the action to the environment\n",
    "            next_states = env_info.vector_observations   # get the next state\n",
    "            rewards = env_info.rewards                   # get the reward\n",
    "            dones = env_info.local_done\n",
    "            beta = next_discount(t, beta_min)\n",
    "            all_states1 = np.concatenate((states[0], states[1]))\n",
    "            all_states2 = np.concatenate((states[1], states[0]))\n",
    "            all_next_states1 = np.concatenate((next_states[0], next_states[1]))\n",
    "            all_next_states2 = np.concatenate((next_states[1], next_states[0]))\n",
    "            agent.step(states[0], all_states1, action1, action2, rewards[0], next_states[0], all_next_states1, dones[0], beta)\n",
    "            agent.step(states[1], all_states2, action2, action1, rewards[1], next_states[1], all_next_states2, dones[1], beta)\n",
    "            states = next_states\n",
    "            score += max(rewards)\n",
    "            if np.any(dones):\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if abort_threshold != None and i_episode > abort_threshold and np.mean(scores_window) < 0.1:\n",
    "            print(\"Aborted\\n\")\n",
    "            break\n",
    "        if np.mean(scores_window)>=0.5:\n",
    "            if not solved:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "                solved = True\n",
    "            if stop_if_solved:\n",
    "                break\n",
    "            \n",
    "    torch.save(agent.actor_local.state_dict(), 'agent_{}_actor_{}.pth'.format(agent.getId(), i_episode-100))\n",
    "    torch.save(agent.critic_local.state_dict(), 'agent_{}_critic_{}.pth'.format(agent.getId(), i_episode-100))\n",
    "    with open('agent_{}_{}.txt'.format(agent.getId(), i_episode-100), 'w') as f:\n",
    "        f.write(agent.summary())\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ddpg import DdpgAgent\n",
    "from src.config import Config\n",
    "\n",
    "configs = [\n",
    "    Config(batch_size=256, use_per=True, add_noise=True, use_bn_actor=True, use_bn_critic=True, noise_type=\"uniform\")\n",
    "]\n",
    "\n",
    "agents = [\n",
    "    DdpgAgent(id=i, state_size=24, action_size=2, config=config) for i, config in enumerate(configs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nRunning experiment with agent #0\n\nMADDPG Agent 0:\nConfig(buffer_size=1000000, batch_size=256, gamma=0.99, tau=0.001, lr_actor=0.0001, lr_critic=0.0001, weight_decay=0, random_seed=0, update_n_step=20, update_n_times=10, actor_hidden_units=[400, 300], critic_hidden_units=[400, 300], noise_type='uniform', add_noise=True, noise_mu=0.0, noise_theta=0.15, noise_sigma=0.2, use_per=True, per_alpha=0.6, per_beta=0.4, per_epsilon=1e-05, use_bn_actor=True, use_bn_critic=True, use_huber_loss=False)Actor(\n  (bns): ModuleList(\n    (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (layers): ModuleList(\n    (0): Linear(in_features=24, out_features=400, bias=True)\n    (1): Linear(in_features=400, out_features=300, bias=True)\n    (2): Linear(in_features=300, out_features=2, bias=True)\n  )\n)Critic(\n  (bn): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (bns): ModuleList(\n    (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (layers): ModuleList(\n    (0): Linear(in_features=24, out_features=400, bias=True)\n    (1): Linear(in_features=402, out_features=300, bias=True)\n    (2): Linear(in_features=300, out_features=1, bias=True)\n  )\n)\n\nEpisode 100\tAverage Score: 0.02\nEpisode 200\tAverage Score: 0.01\nEpisode 300\tAverage Score: 0.02\nEpisode 400\tAverage Score: 0.00\nEpisode 500\tAverage Score: 0.00\nEpisode 600\tAverage Score: 0.00\nEpisode 700\tAverage Score: 0.01\nEpisode 800\tAverage Score: 0.06\nEpisode 900\tAverage Score: 0.12\nEpisode 1000\tAverage Score: 0.19\nEpisode 1069\tAverage Score: 0.50\nEnvironment solved in 969 episodes!\tAverage Score: 0.50\nEpisode 1070\tAverage Score: 0.50\nEnvironment solved in 970 episodes!\tAverage Score: 0.50\nEpisode 1071\tAverage Score: 0.50\nEnvironment solved in 971 episodes!\tAverage Score: 0.50\nEpisode 1072\tAverage Score: 0.50\nEnvironment solved in 972 episodes!\tAverage Score: 0.50\nEpisode 1073\tAverage Score: 0.50\nEnvironment solved in 973 episodes!\tAverage Score: 0.50\nEpisode 1074\tAverage Score: 0.50\nEnvironment solved in 974 episodes!\tAverage Score: 0.50\nEpisode 1075\tAverage Score: 0.52\nEnvironment solved in 975 episodes!\tAverage Score: 0.52\nEpisode 1076\tAverage Score: 0.52\nEnvironment solved in 976 episodes!\tAverage Score: 0.52\nEpisode 1077\tAverage Score: 0.53\nEnvironment solved in 977 episodes!\tAverage Score: 0.53\nEpisode 1078\tAverage Score: 0.52\nEnvironment solved in 978 episodes!\tAverage Score: 0.52\nEpisode 1079\tAverage Score: 0.52\nEnvironment solved in 979 episodes!\tAverage Score: 0.52\nEpisode 1080\tAverage Score: 0.52\nEnvironment solved in 980 episodes!\tAverage Score: 0.52\nEpisode 1081\tAverage Score: 0.53\nEnvironment solved in 981 episodes!\tAverage Score: 0.53\nEpisode 1082\tAverage Score: 0.53\nEnvironment solved in 982 episodes!\tAverage Score: 0.53\nEpisode 1083\tAverage Score: 0.53\nEnvironment solved in 983 episodes!\tAverage Score: 0.53\nEpisode 1084\tAverage Score: 0.53\nEnvironment solved in 984 episodes!\tAverage Score: 0.53\nEpisode 1085\tAverage Score: 0.53\nEnvironment solved in 985 episodes!\tAverage Score: 0.53\nEpisode 1086\tAverage Score: 0.52\nEnvironment solved in 986 episodes!\tAverage Score: 0.52\nEpisode 1087\tAverage Score: 0.54\nEnvironment solved in 987 episodes!\tAverage Score: 0.54\nEpisode 1088\tAverage Score: 0.54\nEnvironment solved in 988 episodes!\tAverage Score: 0.54\nEpisode 1089\tAverage Score: 0.54\nEnvironment solved in 989 episodes!\tAverage Score: 0.54\nEpisode 1090\tAverage Score: 0.54\nEnvironment solved in 990 episodes!\tAverage Score: 0.54\nEpisode 1091\tAverage Score: 0.54\nEnvironment solved in 991 episodes!\tAverage Score: 0.54\nEpisode 1092\tAverage Score: 0.54\nEnvironment solved in 992 episodes!\tAverage Score: 0.54\nEpisode 1093\tAverage Score: 0.54\nEnvironment solved in 993 episodes!\tAverage Score: 0.54\nEpisode 1094\tAverage Score: 0.53\nEnvironment solved in 994 episodes!\tAverage Score: 0.53\nEpisode 1095\tAverage Score: 0.53\nEnvironment solved in 995 episodes!\tAverage Score: 0.53\nEpisode 1096\tAverage Score: 0.53\nEnvironment solved in 996 episodes!\tAverage Score: 0.53\nEpisode 1097\tAverage Score: 0.53\nEnvironment solved in 997 episodes!\tAverage Score: 0.53\nEpisode 1098\tAverage Score: 0.53\nEnvironment solved in 998 episodes!\tAverage Score: 0.53\nEpisode 1099\tAverage Score: 0.53\nEnvironment solved in 999 episodes!\tAverage Score: 0.53\nEpisode 1100\tAverage Score: 0.53\n\nEnvironment solved in 1000 episodes!\tAverage Score: 0.53\nEpisode 1101\tAverage Score: 0.53\nEnvironment solved in 1001 episodes!\tAverage Score: 0.53\nEpisode 1102\tAverage Score: 0.53\nEnvironment solved in 1002 episodes!\tAverage Score: 0.53\nEpisode 1103\tAverage Score: 0.53\nEnvironment solved in 1003 episodes!\tAverage Score: 0.53\nEpisode 1104\tAverage Score: 0.53\nEnvironment solved in 1004 episodes!\tAverage Score: 0.53\nEpisode 1105\tAverage Score: 0.58\nEnvironment solved in 1005 episodes!\tAverage Score: 0.58\nEpisode 1106\tAverage Score: 0.62\nEnvironment solved in 1006 episodes!\tAverage Score: 0.62\nEpisode 1107\tAverage Score: 0.62\nEnvironment solved in 1007 episodes!\tAverage Score: 0.62\nEpisode 1108\tAverage Score: 0.62\nEnvironment solved in 1008 episodes!\tAverage Score: 0.62\nEpisode 1109\tAverage Score: 0.62\nEnvironment solved in 1009 episodes!\tAverage Score: 0.62\nEpisode 1110\tAverage Score: 0.67\nEnvironment solved in 1010 episodes!\tAverage Score: 0.67\nEpisode 1111\tAverage Score: 0.69\nEnvironment solved in 1011 episodes!\tAverage Score: 0.69\nEpisode 1112\tAverage Score: 0.74\nEnvironment solved in 1012 episodes!\tAverage Score: 0.74\nEpisode 1113\tAverage Score: 0.76\nEnvironment solved in 1013 episodes!\tAverage Score: 0.76\nEpisode 1114\tAverage Score: 0.77\nEnvironment solved in 1014 episodes!\tAverage Score: 0.77\nEpisode 1115\tAverage Score: 0.80\nEnvironment solved in 1015 episodes!\tAverage Score: 0.80\nEpisode 1116\tAverage Score: 0.84\nEnvironment solved in 1016 episodes!\tAverage Score: 0.84\nEpisode 1117\tAverage Score: 0.84\nEnvironment solved in 1017 episodes!\tAverage Score: 0.84\nEpisode 1118\tAverage Score: 0.85\nEnvironment solved in 1018 episodes!\tAverage Score: 0.85\nEpisode 1119\tAverage Score: 0.84\nEnvironment solved in 1019 episodes!\tAverage Score: 0.84\nEpisode 1120\tAverage Score: 0.88\nEnvironment solved in 1020 episodes!\tAverage Score: 0.88\nEpisode 1121\tAverage Score: 0.91\nEnvironment solved in 1021 episodes!\tAverage Score: 0.91\nEpisode 1122\tAverage Score: 0.91\nEnvironment solved in 1022 episodes!\tAverage Score: 0.91\nEpisode 1123\tAverage Score: 0.95\nEnvironment solved in 1023 episodes!\tAverage Score: 0.95\nEpisode 1124\tAverage Score: 0.96\nEnvironment solved in 1024 episodes!\tAverage Score: 0.96\nEpisode 1125\tAverage Score: 0.94\nEnvironment solved in 1025 episodes!\tAverage Score: 0.94\nEpisode 1126\tAverage Score: 0.96\nEnvironment solved in 1026 episodes!\tAverage Score: 0.96\nEpisode 1127\tAverage Score: 0.95\nEnvironment solved in 1027 episodes!\tAverage Score: 0.95\nEpisode 1128\tAverage Score: 0.96\nEnvironment solved in 1028 episodes!\tAverage Score: 0.96\nEpisode 1129\tAverage Score: 0.98\nEnvironment solved in 1029 episodes!\tAverage Score: 0.98\nEpisode 1130\tAverage Score: 0.99\nEnvironment solved in 1030 episodes!\tAverage Score: 0.99\nEpisode 1131\tAverage Score: 1.03\nEnvironment solved in 1031 episodes!\tAverage Score: 1.03\nEpisode 1132\tAverage Score: 0.98\nEnvironment solved in 1032 episodes!\tAverage Score: 0.98\nEpisode 1133\tAverage Score: 1.03\nEnvironment solved in 1033 episodes!\tAverage Score: 1.03\nEpisode 1134\tAverage Score: 1.08\nEnvironment solved in 1034 episodes!\tAverage Score: 1.08\nEpisode 1135\tAverage Score: 1.06\nEnvironment solved in 1035 episodes!\tAverage Score: 1.06\nEpisode 1136\tAverage Score: 1.11\nEnvironment solved in 1036 episodes!\tAverage Score: 1.11\nEpisode 1137\tAverage Score: 1.16\nEnvironment solved in 1037 episodes!\tAverage Score: 1.16\nEpisode 1138\tAverage Score: 1.17\nEnvironment solved in 1038 episodes!\tAverage Score: 1.17\nEpisode 1139\tAverage Score: 1.14\nEnvironment solved in 1039 episodes!\tAverage Score: 1.14\nEpisode 1140\tAverage Score: 1.19\nEnvironment solved in 1040 episodes!\tAverage Score: 1.19\nEpisode 1141\tAverage Score: 1.21\nEnvironment solved in 1041 episodes!\tAverage Score: 1.21\nEpisode 1142\tAverage Score: 1.23\nEnvironment solved in 1042 episodes!\tAverage Score: 1.23\nEpisode 1143\tAverage Score: 1.24\nEnvironment solved in 1043 episodes!\tAverage Score: 1.24\nEpisode 1144\tAverage Score: 1.25\nEnvironment solved in 1044 episodes!\tAverage Score: 1.25\nEpisode 1145\tAverage Score: 1.29\nEnvironment solved in 1045 episodes!\tAverage Score: 1.29\nEpisode 1146\tAverage Score: 1.31\nEnvironment solved in 1046 episodes!\tAverage Score: 1.31\nEpisode 1147\tAverage Score: 1.35\nEnvironment solved in 1047 episodes!\tAverage Score: 1.35\nEpisode 1148\tAverage Score: 1.35\nEnvironment solved in 1048 episodes!\tAverage Score: 1.35\nEpisode 1149\tAverage Score: 1.40\nEnvironment solved in 1049 episodes!\tAverage Score: 1.40\nEpisode 1150\tAverage Score: 1.43\nEnvironment solved in 1050 episodes!\tAverage Score: 1.43\nEpisode 1151\tAverage Score: 1.48\nEnvironment solved in 1051 episodes!\tAverage Score: 1.48\nEpisode 1152\tAverage Score: 1.54\nEnvironment solved in 1052 episodes!\tAverage Score: 1.54\nEpisode 1153\tAverage Score: 1.59\nEnvironment solved in 1053 episodes!\tAverage Score: 1.59\nEpisode 1154\tAverage Score: 1.64\nEnvironment solved in 1054 episodes!\tAverage Score: 1.64\nEpisode 1155\tAverage Score: 1.69\nEnvironment solved in 1055 episodes!\tAverage Score: 1.69\nEpisode 1156\tAverage Score: 1.73\nEnvironment solved in 1056 episodes!\tAverage Score: 1.73\nEpisode 1157\tAverage Score: 1.76\nEnvironment solved in 1057 episodes!\tAverage Score: 1.76\nEpisode 1158\tAverage Score: 1.76\nEnvironment solved in 1058 episodes!\tAverage Score: 1.76\nEpisode 1159\tAverage Score: 1.81\nEnvironment solved in 1059 episodes!\tAverage Score: 1.81\nEpisode 1160\tAverage Score: 1.85\nEnvironment solved in 1060 episodes!\tAverage Score: 1.85\nEpisode 1161\tAverage Score: 1.90\nEnvironment solved in 1061 episodes!\tAverage Score: 1.90\nEpisode 1162\tAverage Score: 1.91\nEnvironment solved in 1062 episodes!\tAverage Score: 1.91\nEpisode 1163\tAverage Score: 1.97\nEnvironment solved in 1063 episodes!\tAverage Score: 1.97\nEpisode 1164\tAverage Score: 2.01\nEnvironment solved in 1064 episodes!\tAverage Score: 2.01\nEpisode 1165\tAverage Score: 2.06\nEnvironment solved in 1065 episodes!\tAverage Score: 2.06\nEpisode 1166\tAverage Score: 2.08\nEnvironment solved in 1066 episodes!\tAverage Score: 2.08\nEpisode 1167\tAverage Score: 2.13\nEnvironment solved in 1067 episodes!\tAverage Score: 2.13\nEpisode 1168\tAverage Score: 2.13\nEnvironment solved in 1068 episodes!\tAverage Score: 2.13\nEpisode 1169\tAverage Score: 2.16\nEnvironment solved in 1069 episodes!\tAverage Score: 2.16\nEpisode 1170\tAverage Score: 2.21\nEnvironment solved in 1070 episodes!\tAverage Score: 2.21\nEpisode 1171\tAverage Score: 2.22\nEnvironment solved in 1071 episodes!\tAverage Score: 2.22\nEpisode 1172\tAverage Score: 2.22\nEnvironment solved in 1072 episodes!\tAverage Score: 2.22\nEpisode 1173\tAverage Score: 2.27\nEnvironment solved in 1073 episodes!\tAverage Score: 2.27\nEpisode 1174\tAverage Score: 2.32\nEnvironment solved in 1074 episodes!\tAverage Score: 2.32\nEpisode 1175\tAverage Score: 2.35\nEnvironment solved in 1075 episodes!\tAverage Score: 2.35\nEpisode 1176\tAverage Score: 2.40\nEnvironment solved in 1076 episodes!\tAverage Score: 2.40\nEpisode 1177\tAverage Score: 2.45\nEnvironment solved in 1077 episodes!\tAverage Score: 2.45\nEpisode 1178\tAverage Score: 2.51\nEnvironment solved in 1078 episodes!\tAverage Score: 2.51\nEpisode 1179\tAverage Score: 2.56\nEnvironment solved in 1079 episodes!\tAverage Score: 2.56\nEpisode 1180\tAverage Score: 2.61\nEnvironment solved in 1080 episodes!\tAverage Score: 2.61\nEpisode 1181\tAverage Score: 2.65\nEnvironment solved in 1081 episodes!\tAverage Score: 2.65\nEpisode 1182\tAverage Score: 2.70\nEnvironment solved in 1082 episodes!\tAverage Score: 2.70\nEpisode 1183\tAverage Score: 2.75\nEnvironment solved in 1083 episodes!\tAverage Score: 2.75\nEpisode 1184\tAverage Score: 2.78\nEnvironment solved in 1084 episodes!\tAverage Score: 2.78\nEpisode 1185\tAverage Score: 2.83\nEnvironment solved in 1085 episodes!\tAverage Score: 2.83\nEpisode 1186\tAverage Score: 2.83\nEnvironment solved in 1086 episodes!\tAverage Score: 2.83\nEpisode 1187\tAverage Score: 2.81\nEnvironment solved in 1087 episodes!\tAverage Score: 2.81\nEpisode 1188\tAverage Score: 2.84\nEnvironment solved in 1088 episodes!\tAverage Score: 2.84\nEpisode 1189\tAverage Score: 2.89\nEnvironment solved in 1089 episodes!\tAverage Score: 2.89\nEpisode 1190\tAverage Score: 2.92\nEnvironment solved in 1090 episodes!\tAverage Score: 2.92\nEpisode 1191\tAverage Score: 2.97\nEnvironment solved in 1091 episodes!\tAverage Score: 2.97\nEpisode 1192\tAverage Score: 3.02\nEnvironment solved in 1092 episodes!\tAverage Score: 3.02\nEpisode 1193\tAverage Score: 3.07\nEnvironment solved in 1093 episodes!\tAverage Score: 3.07\nEpisode 1194\tAverage Score: 3.08\nEnvironment solved in 1094 episodes!\tAverage Score: 3.08\nEpisode 1195\tAverage Score: 3.12\nEnvironment solved in 1095 episodes!\tAverage Score: 3.12\nEpisode 1196\tAverage Score: 3.17\nEnvironment solved in 1096 episodes!\tAverage Score: 3.17\nEpisode 1197\tAverage Score: 3.22\nEnvironment solved in 1097 episodes!\tAverage Score: 3.22\nEpisode 1198\tAverage Score: 3.27\nEnvironment solved in 1098 episodes!\tAverage Score: 3.27\nEpisode 1199\tAverage Score: 3.29\nEnvironment solved in 1099 episodes!\tAverage Score: 3.29\nEpisode 1200\tAverage Score: 3.31\n\nEnvironment solved in 1100 episodes!\tAverage Score: 3.31\nEpisode 1201\tAverage Score: 3.36\nEnvironment solved in 1101 episodes!\tAverage Score: 3.36\nEpisode 1202\tAverage Score: 3.40\nEnvironment solved in 1102 episodes!\tAverage Score: 3.40\nEpisode 1203\tAverage Score: 3.46\nEnvironment solved in 1103 episodes!\tAverage Score: 3.46\nEpisode 1204\tAverage Score: 3.51\nEnvironment solved in 1104 episodes!\tAverage Score: 3.51\nEpisode 1205\tAverage Score: 3.51\nEnvironment solved in 1105 episodes!\tAverage Score: 3.51\nEpisode 1206\tAverage Score: 3.52\nEnvironment solved in 1106 episodes!\tAverage Score: 3.52\nEpisode 1207\tAverage Score: 3.56\nEnvironment solved in 1107 episodes!\tAverage Score: 3.56\nEpisode 1208\tAverage Score: 3.57\nEnvironment solved in 1108 episodes!\tAverage Score: 3.57\nEpisode 1209\tAverage Score: 3.58\nEnvironment solved in 1109 episodes!\tAverage Score: 3.58\nEpisode 1210\tAverage Score: 3.59\nEnvironment solved in 1110 episodes!\tAverage Score: 3.59\nEpisode 1211\tAverage Score: 3.61\nEnvironment solved in 1111 episodes!\tAverage Score: 3.61\nEpisode 1212\tAverage Score: 3.58\nEnvironment solved in 1112 episodes!\tAverage Score: 3.58\nEpisode 1213\tAverage Score: 3.58\nEnvironment solved in 1113 episodes!\tAverage Score: 3.58\nEpisode 1214\tAverage Score: 3.63\nEnvironment solved in 1114 episodes!\tAverage Score: 3.63\nEpisode 1215\tAverage Score: 3.59\nEnvironment solved in 1115 episodes!\tAverage Score: 3.59\nEpisode 1216\tAverage Score: 3.56\nEnvironment solved in 1116 episodes!\tAverage Score: 3.56\nEpisode 1217\tAverage Score: 3.61\nEnvironment solved in 1117 episodes!\tAverage Score: 3.61\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-35037706606a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nRunning experiment with agent #{}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetId\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_if_solved\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scores.dump'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-c18df4f327ff>\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(agent, n_episodes, max_t, stop_if_solved, abort_threshold)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mall_next_states2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_states1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_next_states1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_states2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_next_states2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\workspace\\drlnd-collaboration-and-cooperation\\src\\ddpg.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, all_states, action, other_action, reward, next_state, all_nextstates, done, beta)\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_per\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                     \u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                     \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mg:\\workspace\\drlnd-collaboration-and-cooperation\\src\\replay_buffer.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, beta)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpriorities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m# sample the indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpriorities\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpriorities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i, agent in enumerate(agents):\n",
    "    print('\\nRunning experiment with agent #{}\\n'.format(agent.getId()))\n",
    "    print(agent.summary()+\"\\n\")\n",
    "    scores.append(run_experiment(agent, n_episodes=1250, stop_if_solved=False))\n",
    "\n",
    "pickle.dump(scores, open('scores.dump', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = scores[1]\n",
    "sc = sc + sc[-50:] # for plotting only\n",
    "\n",
    "# plot the best scores\n",
    "def movingaverage(interval, window_size):\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(interval, window, 'same')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "window = 50\n",
    "plt.plot(np.arange(len(sc)-window), sc[:-window])\n",
    "plt.plot(np.arange(len(sc)-window), movingaverage(sc,window)[:-window])\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('drlnd': conda)",
   "language": "python",
   "name": "python_defaultSpec_1593350114287"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}