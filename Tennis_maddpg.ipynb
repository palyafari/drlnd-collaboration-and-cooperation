{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\t\nUnity brain name: TennisBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 8\n        Number of stacked Vector Observation: 3\n        Vector Action space type: continuous\n        Vector Action space size (per agent): 2\n        Vector Action descriptions: , \n"
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./environment/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of agents: 2\nSize of each action: 2\nThere are 2 agents. Each observes a state with length: 24\nThe state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.         -6.65278625 -1.5\n -0.          0.          6.83172083  6.         -0.          0.        ]\n"
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[False, False]"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "env_info.local_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(agent, n_episodes=300, max_t=1000, stop_if_solved=True, abort_threshold=None):\n",
    "    \"\"\"MADDPG training.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        agent (Agent): the MADDPG agent\n",
    "        starting_episode (int):  the starting episode number\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        stop_if_solved (boolean): true, if training has to stop when the agent solved the environment\n",
    "        abort_threshold (int): abort training if the agent has scored high enough untis this episode\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores \n",
    "\n",
    "    # beta is only used by agents using PER\n",
    "    beta_min = 0.4\n",
    "    noise_min = 0.1\n",
    "\n",
    "    def next_discount(t, min_value):\n",
    "        return min(1.0, min_value + t * (1.0 - min_value) / max_t)\n",
    "\n",
    "    # noise reduction\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        agent.reset()\n",
    "        states = env_info.vector_observations\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action1 = agent.act(states[0])\n",
    "            action2 = agent.act(states[1])\n",
    "            actions = np.concatenate((action1, action2))\n",
    "            env_info = env.step(actions)[brain_name]           # send the action to the environment\n",
    "            next_states = env_info.vector_observations   # get the next state\n",
    "            rewards = env_info.rewards                   # get the reward\n",
    "            dones = env_info.local_done\n",
    "            beta = next_discount(t, beta_min)\n",
    "            all_states1 = np.concatenate((states[0], states[1]))\n",
    "            all_states2 = np.concatenate((states[1], states[0]))\n",
    "            all_next_states1 = np.concatenate((next_states[0], next_states[1]))\n",
    "            all_next_states2 = np.concatenate((next_states[1], next_states[0]))\n",
    "            agent.step(states[0], all_states1, action1, action2, rewards[0], next_states[0], all_next_states1, dones[0], beta)\n",
    "            agent.step(states[1], all_states2, action2, action1, rewards[1], next_states[1], all_next_states2, dones[1], beta)\n",
    "            states = next_states\n",
    "            score += max(rewards)\n",
    "            if np.any(dones):\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if abort_threshold != None and i_episode > abort_threshold and np.mean(scores_window) < 0.1:\n",
    "            print(\"Aborted\\n\")\n",
    "            break\n",
    "        if np.mean(scores_window)>=0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            if stop_if_solved:\n",
    "                break\n",
    "            \n",
    "    torch.save(agent.actor_local.state_dict(), 'agent_{}_actor_{}.pth'.format(agent.getId(), i_episode-100))\n",
    "    torch.save(agent.critic_local.state_dict(), 'agent_{}_critic_{}.pth'.format(agent.getId(), i_episode-100))\n",
    "    with open('agent_{}_{}.txt'.format(agent.getId(), i_episode-100), 'w') as f:\n",
    "        f.write(agent.summary())\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.maddpg import MaddpgAgent\n",
    "from src.config import Config\n",
    "\n",
    "configs = [\n",
    "    Config(batch_size=256, use_per=True, add_noise=True, use_bn_actor=True, use_bn_critic=False, noise_type=\"uniform\")\n",
    "]\n",
    "\n",
    "agents = [\n",
    "    MaddpgAgent(id=i, state_size=24, action_size=2, config=config) for i, config in enumerate(configs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nRunning experiment with agent #0\n\nMADDPG Agent 0:\nConfig(buffer_size=1000000, batch_size=256, gamma=0.99, tau=0.001, lr_actor=0.0001, lr_critic=0.0001, weight_decay=0, random_seed=0, update_n_step=20, update_n_times=10, actor_hidden_units=[400, 300], critic_hidden_units=[400, 300], noise_type='uniform', add_noise=True, noise_mu=0.0, noise_theta=0.15, noise_sigma=0.2, use_per=True, per_alpha=0.6, per_beta=0.4, per_epsilon=1e-05, use_bn_actor=True, use_bn_critic=False, use_huber_loss=False)Actor(\n  (bns): ModuleList(\n    (0): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (layers): ModuleList(\n    (0): Linear(in_features=24, out_features=400, bias=True)\n    (1): Linear(in_features=400, out_features=300, bias=True)\n    (2): Linear(in_features=300, out_features=2, bias=True)\n  )\n)Critic(\n  (layers): ModuleList(\n    (0): Linear(in_features=50, out_features=400, bias=True)\n    (1): Linear(in_features=402, out_features=300, bias=True)\n    (2): Linear(in_features=300, out_features=1, bias=True)\n  )\n)\n\nEpisode 100\tAverage Score: 0.01\nEpisode 200\tAverage Score: 0.00\nEpisode 300\tAverage Score: 0.01\nEpisode 400\tAverage Score: 0.01\nEpisode 500\tAverage Score: 0.02\nEpisode 600\tAverage Score: 0.02\nEpisode 700\tAverage Score: 0.02\nEpisode 800\tAverage Score: 0.04\nEpisode 900\tAverage Score: 0.03\nEpisode 1000\tAverage Score: 0.04\nEpisode 1100\tAverage Score: 0.03\nEpisode 1200\tAverage Score: 0.05\nEpisode 1300\tAverage Score: 0.08\nEpisode 1400\tAverage Score: 0.12\nEpisode 1500\tAverage Score: 0.05\nEpisode 1600\tAverage Score: 0.05\nEpisode 1700\tAverage Score: 0.15\nEpisode 1800\tAverage Score: 0.17\nEpisode 1900\tAverage Score: 0.18\nEpisode 2000\tAverage Score: 0.31\nEpisode 2038\tAverage Score: 0.50\nEnvironment solved in 1938 episodes!\tAverage Score: 0.50\n"
    }
   ],
   "source": [
    "scores = []\n",
    "for i, agent in enumerate(agents):\n",
    "    print('\\nRunning experiment with agent #{}\\n'.format(agent.getId()))\n",
    "    print(agent.summary()+\"\\n\")\n",
    "    scores.append(run_experiment(agent, n_episodes=2500, stop_if_solved=True))\n",
    "\n",
    "pickle.dump(scores, open('scores.dump', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}